{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ResNet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOD9qA7zG6oy0dwxdYSoSD7"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"sOAzzCLr2Z3Q","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rx7lF4wj2bMv","colab_type":"text"},"source":["#RESNET:\n","\n","Được ra đời sau GoogLeNet 1 năm đây được xem như mô hình mạng CNN tốt nhất hiện tại và cũng là mô hình mạng CNN sâu nhất.\n","\n","Trước đó chúng ta được biết mạng GoogLeNet có 22 Layer. Vậy thêm Layer vào liệu có vấn đề gì không Liệu càng nhiều Layer giải quyết bài toán phức tạp càng tốt hơn?\n","\n","Câu trả lời là không. Hãy xem đồ thị sau đây:\n","\n","![alt text](http://teleported.in/post_imgs/12-deepnet-errors.png)\n","\n","Có thể thấy với 20 Layer thậm chí độ lỗi của mô hình còn thấp hơn so với 50 Layer vậy tại sao lại có điều này Liệu đó là kết quả của việc Over fitting.\n","Thực ra đây là hậu quả của việc Vanishing gradient do mô hình chúng ta quá sâu.\n","\n","Vanishing gradient problem là bài toán ta phải đối mặt khi muốn xây dụng mô hình sâu hơn mô hình càng sâu thì càng dễ bị vanishing gradient. Đây là hiện tượng gradient rất nhở có thể gần bằng 0 điều này sẽ làm cho việc upadate các weight và bias tại từng layer qua quá trình back propag sẽ không thể cập nhật hay nói cách khác các Layer ấy sẽ không còn được học nữa. Điều này làm cho mô hình của chúng ta nếu may mắn thì sai ít còn nếu mô hình quá sâu điều này sẽ làm cho mô hình sai hoàn toàn. \n","\n","Theo quá trình Back-Propagation thì gradient của lớp này sẽ bằng tích của các lớp trước đó theo chain-rule và nếu các gradient trước đó < 1 thì mạng càng sâu lớp thì thông tin truyền đi nhằm mục đích update càng ngày càng nhỏ lại và đến mức gần như biến mất. Để giải quyết vấn đề này các mô hình trước đã sử dụng relu thay cho các hàm tanh hoặc sigmod nhưng điều này cũng chỉ áp dụng khi số lượng Layer chưa quá lớn hay như GoogLeNet có áp dụng thêm classifier phụ để giải quyết vấn đề này.\n","\n","Ngoài những cách trên có cách hiệu quả nhât hiện nay là dùng Residual Block.\n","\n","I) Residual Block:\n","\n","![alt text](http://teleported.in/post_imgs/12-residual-net.png)\n","- Đây là thành phần nhỏ của mô hình mạng resNet ( cũng giống như inception block)\n","- ở cách thức tiếp cận, thay vì chỉ dùng Relu hoặc sử dụng classifier phụ ở các mô hình VGG hay GoogLeNet thì ở đây ta sử dụng 1 cách khác đó là sẽ lấy input của 1 blocks cộng nó vào output hay nói cách khác thì ta tạo 1 shortcut , 1 đường tắt nối các lớp phía trước với các lớp sau VD nối lớp L và lớp L+5. Làm như thế để làm gì ? Như được biết thì lớp cằng sâu thì vấn đề vanish gradient càng xảy ra nhanh hơn như vậy VD :\n","\n","Lớp L lúc này gradient sấp xỉ bằng 0 khi đi qua hàng loạt các lớp trước đó như sẽ gradient coi như đã biến mất nhưng khi đã tạo shortcut thì lúc này lớp L vẫn có thể được trực tiếp cập nhật từ lớp L+5 lúc mà gradient vẫn còn đủ lớn và điều này khiến cho mạng của chúng ta dù sâu nhưng các lớp vẫn được cập nhật và tiến trình cập nhật này sẽ ưu tiên cập nhật cho phần shortcut nếu như có hiện tượng vanishing gradient và lúc này những lớp từ L+1 đến L+4 được coi như là lớp dư và thuật toán sẽ tự điều chỉnh những weight của lớp này xuống 0 và đi từ lớp L đến lớp L+5 ta sẽ coi như nó đi qua 1 idendity mapping ( F(x) = x ). Có thể thấy với cách này ta sẽ không còn phải lo lắng đến việc liệu có biến mất gradient hay không vì chúng ta sẽ tạo ra các short cut để giúp  \n","\n","\n","III) ResNet:\n","\n","- Với ý tưởng của Residual Block thì ta sẽ buil được mô hình mạng ResNet với chiều sâu có để đạt tới hàng trăm hàng nghìn Layer phục vụ cho từng mục đích mà không lo tới việc vanishing gradient.\n","\n","![alt text](http://teleported.in/post_imgs/12-resnet-vgg.png)\n","\n","![alt text](https://i.imgur.com/MCFUlvY.png)"]}]}